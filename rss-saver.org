* rss-saver
A script that saves my blog posts from [[https://world.hey.com/adam.james/]]

** readme
#+begin_src markdown :tangle ./readme.md
# RSS-saver
A simple Clojure (Babashka) script to save world@hey.com blog posts locally.

I use hey.com's blog feature to write blog posts to help me clarify and improve my own thinking about life, design, and programming. It's a cool feature to a nice service, but I worry that I may not be able to retrieve my posts in the event of service shutdown, or if I move on to another email provider in the future.

This script is meant to be run automatically every night (or so). It downloads the feed.atom xml file from the provided URL, checks for any changes from the previous download, and saves new entries.

## Status
Working 'skeleton'. I wrote this during one of my streams, and it works with my (hardcoded) blog RSS URL. I have to clean up the node transformation and open up the feature set slightly:

 - option to change the feed URL to other hey URLs. I am NOT worrying about other RSS feed formats at this time. Not sure if they are standardised anyway.
 - add a few output options: .txt, .md, .org, .html ?? Not entirely sure which yet.
 - make a proper CLI w/ help printout
 - make sure a GraalVM native-image build is possible

#+end_src

* What is RSS?
I've never properly learned what RSS actually is. Here's my RSS link that I'll be using:

[[https://world.hey.com/adam.james/feed.atom]]

I assume that the atom file at that address is automatically updated any time a post is created, and I assume it's just XML with all of the blog's content.

What I think RSS does:

Every time the site updates, the feed.atom file is re-generated with the newest content appended. Then, the RSS reader is a separate app that polls feed.atom URLs, downloads them, and parses/displays the contents according to the app's design.

* Design
This is a simple script that only saves my world.hey.com blog posts to text files. Run the script once, it will grab feed.atom from the URL, parse it, and save each post and its content as plaintext.

To expand that minimal scope, I want:

- basic diffing approach which saves the previous feed.atom file so that when the script runs, it can compare the newest feed from the previous, and only saves new articles.
- some articles have images, I would like to be able to download those images and save them inside a folder generated per post.
- I want to emit the posts in either markdown OR org files, not sure yet.

* main
** ns
Need xml parsing. Might need to build a zipper to edit nodes.

I'll want to run it as a CLI, and probably want to turn the project into another GRAALVM native image example, so I'll need tools.cli as well.

#+begin_src clojure :tangle ./rss-saver.clj
(ns rss-saver.main
  (:require [clojure.string :as str]
            [clojure.data.xml :as xml]
            [clojure.java.io :as io]
            [clojure.zip :as zip]
            [clojure.java.shell :as sh :refer [sh]]
            [clojure.tools.cli :as cli]))

#+end_src

** get-feed-proto
This is not part of the final script, but was useful for the skeleton script.

#+begin_src clojure
(def feed-url "https://world.hey.com/adam.james/feed.atom")
(def current-feed-str (slurp feed-url))

(defn init-previous!
  "Saves current-feed-str to previous-feed.atom when no previous-feed.atom file exists."
  []
  (when-not (.exists (io/file "previous-feed.atom"))
    (spit "previous-feed.atom" "")))

(init-previous!)

(def previous-feed-str (slurp "previous-feed.atom"))

#+end_src

** zipper tools
#+begin_src clojure :tangle ./rss-saver.clj
;; https://ravi.pckl.me/short/functional-xml-editing-using-zippers-in-clojure/
(defn edit-nodes
  [zipper matcher editor]
  (loop [loc zipper]
    (if (zip/end? loc)
      (zip/root loc)
      (if-let [matcher-result (matcher loc)]
        (let [new-loc (zip/edit loc editor)]
          (if (not (= (zip/node new-loc) (zip/node loc)))
            (recur (zip/next new-loc))
            (recur (zip/next loc))))
        (recur (zip/next loc))))))

(defn remove-nodes
  [zipper matcher]
  (loop [loc zipper]
    (if (zip/end? loc)
      (zip/root loc)
      (if-let [matcher-result (matcher loc)]
        (let [new-loc (zip/remove loc)]
          (recur (zip/next new-loc)))
        (recur (zip/next loc))))))

(defn get-nodes
  [zipper matcher]
  (loop [loc zipper
         acc []]
    (if (zip/end? loc)
      acc
      (if (matcher loc)
        (recur (zip/next loc) (conj acc (zip/node loc)))
        (recur (zip/next loc) acc)))))

(defn match-entry?
  [loc]
  (let [node (zip/node loc)
        {:keys [tag]} node]
    (= tag :entry)))
  
#+end_src

** entry nodes
#+begin_src clojure :tangle ./rss-saver.clj
(defn feed-str->entries
  "Returns a sequence of parsed article entry nodes from an XML feed string."
  [s]
  (-> s
      (xml/parse-str {:namespace-aware false})
      zip/xml-zip
      (get-nodes match-entry?)))

#+end_src

** node-transforms
*** normalize
#+begin_src clojure :tangle ./rss-saver.clj
(defn normalize-entry
  "Normalizes the entry node by flattening content into a map."
  [entry]
  (let [content (filter map? (:content entry))
        f (fn [{:keys [tag content] :as node}]
            (let [val (cond (= tag :link) (get-in node [:attrs :href])
                            :else (first content))]
                {tag val}))
        author-map (->> content
                        (filter #(= (:tag %) :author))
                        first :content
                        (filter map?)
                        (map f)
                        (apply merge))]
   (apply merge (conj
                 (map f (remove #(= (:tag %) :author) content))
                 author-map))))
#+end_src

*** clean-html
#+begin_src clojure :tangle ./rss-saver.clj
(defn match-tag
  [k]
  (fn
    [loc]
    (let [node (zip/node loc)
          {:keys [tag]} node]
      (= tag k))))

(defn wrap-strs-in-p-tags
  [node]
  (let [f (fn [item]
            (if (string? item)
              {:tag :p :attrs {} :content [item]}
              item))
        new-content (->> node
                         :content
                         (map f))]
    (assoc node :content new-content)))

(defn convert-to-p-tag
  [node]
  (assoc node :tag :p))

(defn unwrap-img-from-figure
  [node]
  (let [img-node (-> node
                 zip/xml-zip
                 (get-nodes (match-tag :img))
                 first)
        new-attrs (-> img-node
                      :attrs
                      (dissoc :srcset :decoding :loading))]
    (assoc img-node :attrs new-attrs)))

(defn clean-html
  "Clean up the html string from the feed."
  [s]
  (let [s (-> s
              (str/replace "<br>" "<br></br>")
              (str/replace #"<img[\w\W]+?>" #(str %1 "</img>")))]
    (-> s
        (xml/parse-str {:namespace-aware false})
        zip/xml-zip
        (edit-nodes (match-tag :figure) unwrap-img-from-figure)
        xml/emit-str
        (str/replace #"<\?xml[\w\W]+?>" ""))))

#+end_src

*** basic-html
#+begin_src clojure :tangle ./rss-saver.clj
(defn readable-date
  [s]
  (as-> s s
    (str/split s #"[a-zA-Z]")
    (str/join " " s)))

(defn entry->html
  "Converts a parsed XML entry node into an html document.
  The generated html string does not reformat the html from the feed."
  [entry]
  (let [entry (normalize-entry entry)]
    {:id (:id entry)
     :post (format "
<!DOCTYPE html>
<html lang=\"en\">
  <head>
    <meta charset=\"utf-8\"/>
    <title>%s</title>
  </head>
  <body>
    <p><strong>Author:</strong> %s</p>
    <p><strong>email:</strong> %s</p>
    <p><strong>Published:</strong> %s</p>
    <p><strong>Updated:</strong> %s</p>
    <a href=\"%s\"><h1>%s</h1></a>
    %s
  </body>
</html>"
                   (:title entry)
                   (:name entry)
                   (:email entry)
                   (readable-date (:published entry))
                   (readable-date (:updated entry))
                   (:link entry)
                   (:title entry)
                   (clean-html (:content entry)))}))
#+end_src

** markdown
#+begin_src clojure :tangle ./rss-saver.clj


#+end_src

** CLI
#+begin_src clojure :tangle ./rss-saver.clj
(def cli-options
  [["-h" "--help"]
   ["-u" "--url URL" "The URL of the RSS feed you want to save."]
   ["-d" "--dir DIR" "The directory where articles will be saved."
    :default "./posts"]
   ["-c" "--clear" "Clear the cached copy of the previous feed."]])

(defn clear!
  [opts]
  (let [prev-fname (str (:dir opts) "/" "previous-feed.atom")]
    (sh "rm" "-f" prev-fname)))

(defn save!
  [opts]
  (let [cur-str (slurp (:url opts))
        prev-fname (str (:dir opts) "/" "previous-feed.atom")
        prev-str (when (.exists (io/file prev-fname))
                   (slurp prev-fname))
        prev (when prev-str (feed-str->entries prev-str))
        cur (feed-str->entries cur-str)
        entries (remove (into #{} prev) cur)]
    (if (> (count entries) 0)
      (do
        (println (str "Handling " (count entries) " entries."))
        (sh "mkdir" "-p" (:dir opts))
        (doseq [{:keys [id post]} (mapv entry->html entries)]
          (let [fname (str
                       (:dir opts) "/"
                       (second (str/split id #"/")) ".html")]
            (spit fname post)))
        (spit prev-fname cur-str))
      (println "No changes found in feed."))))

(defn -main
  [& args]
  (let [parsed (cli/parse-opts args cli-options)
        opts (:options parsed)]
    (cond
      (:help opts)
      (println (str "Usage:" "\n" (:summary parsed)))

      (nil? (:url opts))
      (println "Please specify feed URL.")

      :else
      (do
        (when (:clear opts) (clear! opts))
        (save! opts)))))

(apply -main *command-line-args*)
(shutdown-agents)
#+end_src
